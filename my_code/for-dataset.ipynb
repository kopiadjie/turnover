{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kode awal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "read = pd.read_csv('terjemahandataset.csv')\n",
    "df = read.iloc[0:100]\n",
    "df = pd.DataFrame(df, columns=['airline_sentiment', 'text_terjemahan'])\n",
    "dft = pd.DataFrame(df, columns=['text_terjemahan'])\n",
    "dft.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize as token_kata\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize as token_kata\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def ubah_ke_huruf_kecil(dataframe, column_name):\n",
    "    dataframe[column_name] = dataframe[column_name].str.lower()\n",
    "    return dataframe\n",
    "\n",
    "# Fungsi untuk menghapus karakte-karakter spesial twitter(X) dari data hasil scrapping\n",
    "def bersihkan_karakter_twitter(text):\n",
    "    text = text.replace('\\\\t', \" \").replace('\\\\n', \" \").replace('\\\\u', \" \").replace('\\\\', \"\")\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    text = ' '.join(re.sub(r\"([@#][A-Za-z0-9_]+)|(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "\n",
    "# Fungsi untuk menghapus angka untuk penyederhanaan data clear\n",
    "def hapus_angka(text):\n",
    "    return re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "# Fungsi untuk menghapus tanda baca seperti titik, koma, tanda seru, dll.\n",
    "def hapus_tandabaca(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "# Fungsi untuk menghapus whitespace atau spasi yang ada di awal dan akhir teks.\n",
    "def hapus_spasi_awalakhir(text):\n",
    "    return text.strip()\n",
    "\n",
    "# Fungsi untuk mengganti spasi berturut turut dengan satu spasi tunggal.\n",
    "def ganti_spasi_tunggal(text):\n",
    "    return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "# Fungsi untuk menghapus karakter tunggal yang berdiri sendiri di dalam teks\n",
    "def hapus_karakter_tunggal(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "# Fungsi untuk membaca file JSON yang berisi kamus slang (bahasa gaul) dan menyimpannya ke dalam bentuk dictionary\n",
    "def muat_kamus_slang(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "# Fungsi untuk menggantikan bahasa gaul di dalam teks dengan padanan yang lebih baku berdasarkan kamus slang.\n",
    "def slang_ke_baku(text, kamus_bahasa_gaul):\n",
    "    return \" \".join(kamus_bahasa_gaul.get(word, word) for word in text.split())\n",
    "\n",
    "# Stemming\n",
    "pengolahdata = StemmerFactory() # Digunakan untuk membuat objek stemmer.\n",
    "stemmer = pengolahdata.create_stemmer() # Membuat objek stemmer yang dapat digunakan untuk melakukan proses stemming.\n",
    "\n",
    "# Fungsi untuk mengubah teks ke dalam bentuk token\n",
    "def bungkus_tokenisasi_kata(text):\n",
    "    return text.split()\n",
    "# Fungsi untuk mengubah setiap kata ke bentuk dasarnya.\n",
    "def sederhanakan_teks(tokens):\n",
    "    # Gabungkan token menjadi string dan lakukan stemming\n",
    "    return ' '.join([stemmer.stem(token) for token in tokens])\n",
    "\n",
    "\n",
    "\n",
    "class StopWordsIndo:\n",
    "    def __init__(self, stopwords_file):\n",
    "        self.stopwords = self.olah_stopword(stopwords_file)\n",
    "    \n",
    "    def olah_stopword(self, stopwords_file):\n",
    "        with open(stopwords_file, 'r', encoding='utf-8') as f:\n",
    "            return set(f.read().splitlines())\n",
    "    \n",
    "    def hapus_stopwords(self, text):\n",
    "        pisahkan_kata = text.split()\n",
    "        kata_bersih = [word for word in pisahkan_kata if word not in self.stopwords and len(word) > 3]\n",
    "        return \" \".join(kata_bersih)\n",
    "class KamusFilter:\n",
    "    def __init__(self, kamus_file):\n",
    "        self.term_dict = self.baca_kamus(kamus_file)\n",
    "\n",
    "    def baca_kamus(self, kamus_file):\n",
    "        try:\n",
    "            with open(kamus_file, 'r', encoding='utf-8') as file:\n",
    "                return set(file.read().splitlines())\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {kamus_file} tidak ditemukan.\")\n",
    "            return set()\n",
    "\n",
    "    def hapus_bukan_id(self, document):\n",
    "        return [term for term in document if term in self.term_dict]\n",
    "\n",
    "\n",
    "\n",
    "# Fungsi untuk memuat leksikon sentimen\n",
    "def load_lexicon(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "    \n",
    "\n",
    "dft['text_terjemahan'] = ubah_ke_huruf_kecil(dft, 'text_terjemahan')\n",
    "dft['text_terjemahan'] = dft['text_terjemahan'].apply(bersihkan_karakter_twitter)\n",
    "dft['text_terjemahan'] = dft['text_terjemahan'].apply(hapus_angka)\n",
    "dft['text_terjemahan'] = dft['text_terjemahan'].apply(hapus_tandabaca)\n",
    "dft['text_terjemahan'] = dft['text_terjemahan'].apply(hapus_spasi_awalakhir)\n",
    "dft['text_terjemahan'] = dft['text_terjemahan'].apply(ganti_spasi_tunggal)\n",
    "dft['text_terjemahan'] = dft['text_terjemahan'].apply(hapus_karakter_tunggal)\n",
    "\n",
    "# Load dan replace slang\n",
    "kamus_bahasa_gaul = muat_kamus_slang(\"txt/kamusSlang.json\")\n",
    "dft['text_terjemahan'] = dft['text_terjemahan'].apply(lambda x: slang_ke_baku(x, kamus_bahasa_gaul))\n",
    "\n",
    "# Tokenisasi\n",
    "dft['text_terjemahan'] = dft['text_terjemahan'].apply(bungkus_tokenisasi_kata)\n",
    "\n",
    "# Stemming\n",
    "dft['text_terjemahan'] = dft['text_terjemahan'].apply(sederhanakan_teks)\n",
    "\n",
    "# Inisialisasi stopword dan kamus filter\n",
    "stopwords_processor = StopWordsIndo('txt/stopwords.txt')\n",
    "kamus_filter = KamusFilter(\"txt/kamusIndonesia.txt\")\n",
    "\n",
    "# Hapus stopword\n",
    "dft['stopwords'] = dft['text_terjemahan'].apply(lambda x: stopwords_processor.hapus_stopwords(x))\n",
    "\n",
    "# Filter term non-indonesia\n",
    "dft['filtered'] = dft['stopwords'].apply(lambda x: kamus_filter.hapus_bukan_id(x.split()))\n",
    "\n",
    "# Load leksikon positif dan negatif\n",
    "pos_lexicon = load_lexicon('leksikon/leksikon-pos.json')\n",
    "neg_lexicon = load_lexicon('leksikon/leksikon-neg.json')\n",
    "\n",
    "def hitung_sentimen_berdasarkan_leksikon(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    pos_count = sum(1 for word in text.split() if word in pos_lexicon)\n",
    "    neg_count = sum(1 for word in text.split() if word in neg_lexicon)\n",
    "    if pos_count > neg_count:\n",
    "        return 'Positif', 1  # Sentimen positif dan skor\n",
    "    elif neg_count > pos_count:\n",
    "        return 'Negatif', -1  # Sentimen negatif dan skor\n",
    "    else:\n",
    "        return 'Netral', 0  # Sentimen netral dan skor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET TERJEMAHANDATASET.CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize as token_kata\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Fungsi preprocessing\n",
    "pengolahdata = StemmerFactory()\n",
    "stemmer = pengolahdata.create_stemmer()\n",
    "\n",
    "def preprocessing(text, slang_dict, stopwords, kamus_indonesia):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\\\t|\\\\n|\\\\u|\\\\|http[s]?://\\\\S+|[@#][A-Za-z0-9_]+\", \" \", text)\n",
    "    text = re.sub(r\"\\\\d+\", \"\", text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Menghapus tanda baca\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Menghapus karakter selain huruf dan spasi\n",
    "    text = re.sub(r\"\\\\s+\", ' ', text).strip()  # Menghapus whitespace tambahan\n",
    "    text = ' '.join([slang_dict.get(word, word) for word in text.split()])\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split() if word not in stopwords and len(word) > 3 and word in kamus_indonesia])\n",
    "    return text\n",
    "\n",
    "# Muat data\n",
    "def load_data():\n",
    "    df = pd.read_csv('terjemahandataset.csv').iloc[0:100]\n",
    "    return pd.DataFrame(df, columns=['airline_sentiment', 'text_terjemahan'])\n",
    "\n",
    "def load_lexicon(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return set(json.load(file))\n",
    "\n",
    "def load_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return set(file.read().splitlines())\n",
    "\n",
    "def hitung_sentimen(text, pos_lexicon, neg_lexicon):\n",
    "    pos_count = sum(1 for word in text.split() if word in pos_lexicon)\n",
    "    neg_count = sum(1 for word in text.split() if word in neg_lexicon)\n",
    "    if pos_count > neg_count:\n",
    "        return 'positive', 1\n",
    "    elif neg_count > pos_count:\n",
    "        return 'negative', -1\n",
    "    else:\n",
    "        return 'neutral', 0\n",
    "\n",
    "\n",
    "\n",
    "# Muat sumber daya\n",
    "slang_dict = json.load(open(\"txt/kamusSlang.json\", \"r\", encoding=\"utf-8\"))\n",
    "stopwords = load_file('txt/stopwords.txt')\n",
    "kamus_indonesia = load_file('txt/kamusIndonesia.txt')\n",
    "pos_lexicon = load_lexicon('leksikon/leksikon-pos.json')\n",
    "neg_lexicon = load_lexicon('leksikon/leksikon-neg.json')\n",
    "\n",
    "# Proses data\n",
    "df = load_data()\n",
    "df['ulasan'] = df['text_terjemahan'].apply(lambda x: preprocessing(x, slang_dict, stopwords, kamus_indonesia))\n",
    "\n",
    "# Hapus baris yang memiliki nilai kosong (termasuk yang berisi spasi atau karakter non-huruf)\n",
    "df = df[df['ulasan'].str.strip().astype(bool)]  # Hanya baris yang tidak kosong setelah strip dan konversi ke bool\n",
    "\n",
    "# Lanjutkan dengan analisis\n",
    "df[['prediksi', 'prediksi-skor']] = df['ulasan'].apply(lambda x: pd.Series(hitung_sentimen(x, pos_lexicon, neg_lexicon)))\n",
    "\n",
    "accuracy = (df['airline_sentiment'] == df['prediksi']).mean()\n",
    "print(f\"Akurasinya: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET GLASSDOOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"glassdoor_reviews/uji-klasifikasi-sentimen-100.csv\")\n",
    "# df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Fungsi preprocessing\n",
    "pengolahdata = StemmerFactory()\n",
    "stemmer = pengolahdata.create_stemmer()\n",
    "\n",
    "# def preprocessing(text, slang_dict, stopwords, kamus_indonesia):\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r\"\\\\t|\\\\n|\\\\u|\\\\|http[s]?://\\\\S+|[@#][A-Za-z0-9_]+\", \" \", text)\n",
    "#     text = re.sub(r\"\\\\d+\", \"\", text)\n",
    "#     text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Menghapus tanda baca\n",
    "#     text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Menghapus karakter selain huruf dan spasi\n",
    "#     text = re.sub(r\"\\\\s+\", ' ', text).strip()  # Menghapus whitespace tambahan\n",
    "#     text = ' '.join([slang_dict.get(word, word) for word in text.split()])\n",
    "#     text = ' '.join([stemmer.stem(word) for word in text.split() if word not in stopwords and len(word) > 3 and word in kamus_indonesia])\n",
    "#     return text\n",
    "def preprocessing(text, slang_dict, stopwords, kamus_indonesia):\n",
    "    text = text.lower()  # Case folding\n",
    "    text = re.sub(r\"\\\\t|\\\\n|\\\\u|\\\\|http[s]?://\\\\S+|[@#][A-Za-z0-9_]+\", \" \", text)  # Menghapus karakter khusus\n",
    "    text = re.sub(r\"\\\\d+\", \"\", text)  # Menghapus angka\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Menghapus tanda baca (pakai import string)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Menghapus karakter selain huruf dan spasi\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text) # Menghapus satu huruf (besar/kecil)\n",
    "    text = re.sub(r\"\\\\s+\", ' ', text).strip()  # Menghapus spasi ganda\n",
    "    text = ' '.join([slang_dict.get(word, word) for word in text.split()]) # Normalisasi (pakai kamus slang)\n",
    "    text = word_tokenize(text) # Tokenisasi (sebelum stemming)\n",
    "    text = ' '.join([stemmer.stem(word) for word in text if word not in stopwords and len(word) > 3 and word in kamus_indonesia]) # Stemming, stopwords dan filter\n",
    "    return text\n",
    "\n",
    "# Muat data\n",
    "def load_data():\n",
    "    df = pd.read_csv('csv/uji-klasifikasi-sentimen.csv') #.iloc[0:100]\n",
    "    return pd.DataFrame(df)\n",
    "\n",
    "def load_lexicon(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return set(json.load(file))\n",
    "\n",
    "def load_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return set(file.read().splitlines())\n",
    "\n",
    "def hitung_sentimen(text, pos_lexicon, neg_lexicon):\n",
    "    pos_count = sum(1 for word in text.split() if word in pos_lexicon)\n",
    "    neg_count = sum(1 for word in text.split() if word in neg_lexicon)\n",
    "    if pos_count > neg_count:\n",
    "        return 'Positif', 1\n",
    "    elif neg_count > pos_count:\n",
    "        return 'Negatif', -1\n",
    "    else:\n",
    "        return 'Netral', 0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1678 entries, 0 to 1677\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentimen  1678 non-null   object\n",
      " 1   ulasan    1678 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 26.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Muat sumber daya\n",
    "slang_dict = json.load(open(\"txt/kamusSlang.json\", \"r\", encoding=\"utf-8\"))\n",
    "stopwords = load_file('txt/stopwords.txt')\n",
    "kamus_indonesia = load_file('txt/kamusIndonesia.txt')\n",
    "pos_lexicon = load_lexicon('leksikon/leksikon-pos.json')\n",
    "neg_lexicon = load_lexicon('leksikon/leksikon-neg.json')\n",
    "\n",
    "# Proses data\n",
    "df = load_data()\n",
    "df = df.dropna(subset=['sentimen'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasinya: 50.90%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df['ulasan'] = df['ulasan'].apply(lambda x: preprocessing(x, slang_dict, stopwords, kamus_indonesia))\n",
    "\n",
    "# Hapus baris yang memiliki nilai kosong (termasuk yang berisi spasi atau karakter non-huruf)\n",
    "df = df[df['ulasan'].str.strip().astype(bool)] \n",
    "\n",
    "# Lanjutkan dengan analisis\n",
    "df[['prediksi', 'prediksi-skor']] = df['ulasan'].apply(lambda x: pd.Series(hitung_sentimen(x, pos_lexicon, neg_lexicon)))\n",
    "\n",
    "accuracy = (df['sentimen'] == df['prediksi']).mean()\n",
    "print(f\"Akurasinya: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentimen</th>\n",
       "      <th>ulasan</th>\n",
       "      <th>prediksi</th>\n",
       "      <th>prediksi-skor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1673</th>\n",
       "      <td>Positif</td>\n",
       "      <td>mahasiswa manajemen fleksibel bantu toko rumah...</td>\n",
       "      <td>Positif</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>Negatif</td>\n",
       "      <td>dasar dasar inti lupa rekan</td>\n",
       "      <td>Positif</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1675</th>\n",
       "      <td>Positif</td>\n",
       "      <td>kolega main upah toko lokal dekat bagus</td>\n",
       "      <td>Positif</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676</th>\n",
       "      <td>Positif</td>\n",
       "      <td>anggota staf ramah bantu temu produk makan pakai</td>\n",
       "      <td>Positif</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>Negatif</td>\n",
       "      <td>senjang gaji zona manajemen tingkat rekan reka...</td>\n",
       "      <td>Positif</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentimen                                             ulasan prediksi  \\\n",
       "1673  Positif  mahasiswa manajemen fleksibel bantu toko rumah...  Positif   \n",
       "1674  Negatif                        dasar dasar inti lupa rekan  Positif   \n",
       "1675  Positif            kolega main upah toko lokal dekat bagus  Positif   \n",
       "1676  Positif   anggota staf ramah bantu temu produk makan pakai  Positif   \n",
       "1677  Negatif  senjang gaji zona manajemen tingkat rekan reka...  Positif   \n",
       "\n",
       "      prediksi-skor  \n",
       "1673              0  \n",
       "1674              0  \n",
       "1675              0  \n",
       "1676              1  \n",
       "1677              1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>firm</th>\n",
       "      <th>recommend</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFH-Wealth-Management</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>Poor salaries, poor training and communication.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFH-Wealth-Management</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>Poor salary which doesn't improve much with pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFH-Wealth-Management</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>Very low salary, poor working conditions, very...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFH-Wealth-Management</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>No career progression and salary is poor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFH-Wealth-Management</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>Abysmal pay, around minimum wage. No actual tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>AJ-Bell</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Great offices and benefits on offer.\\r\\nGreat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>AJ-Bell</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>- Soul-less working environment\\r\\n- Inconveni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>AJ-Bell</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Great working atmosphere, lots of chance for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>AJ-Bell</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>Too many to list. \\r\\nHR is biased and sides w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>AJ-Bell</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>Alot of false hope given to make you think you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     firm recommend  \\\n",
       "0   AFH-Wealth-Management   Negatif   \n",
       "1   AFH-Wealth-Management   Negatif   \n",
       "2   AFH-Wealth-Management   Negatif   \n",
       "3   AFH-Wealth-Management   Negatif   \n",
       "4   AFH-Wealth-Management   Negatif   \n",
       "..                    ...       ...   \n",
       "95                AJ-Bell   Positif   \n",
       "96                AJ-Bell   Negatif   \n",
       "97                AJ-Bell   Positif   \n",
       "98                AJ-Bell   Negatif   \n",
       "99                AJ-Bell   Negatif   \n",
       "\n",
       "                                               review  \n",
       "0     Poor salaries, poor training and communication.  \n",
       "1   Poor salary which doesn't improve much with pr...  \n",
       "2   Very low salary, poor working conditions, very...  \n",
       "3            No career progression and salary is poor  \n",
       "4   Abysmal pay, around minimum wage. No actual tr...  \n",
       "..                                                ...  \n",
       "95  Great offices and benefits on offer.\\r\\nGreat ...  \n",
       "96  - Soul-less working environment\\r\\n- Inconveni...  \n",
       "97  Great working atmosphere, lots of chance for p...  \n",
       "98  Too many to list. \\r\\nHR is biased and sides w...  \n",
       "99  Alot of false hope given to make you think you...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look = pd.read_csv('csv/new_df-ada-netral.csv')\n",
    "look.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
