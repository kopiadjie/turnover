{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definisi fungsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "def load_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return set(file.read().splitlines())\n",
    "    \n",
    "def load_lexicon(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return set(json.load(file))  \n",
    "    \n",
    "slang_dict = json.load(open(\"txt/kamusSlang.json\", \"r\", encoding=\"utf-8\"))\n",
    "stopwords = load_file('txt/stopwords-1.txt')\n",
    "kamus_indonesia = load_file('txt/kamusIndonesia.txt')\n",
    "pos_lexicon = load_lexicon('leksikon/leksikon-pos.json')\n",
    "neg_lexicon = load_lexicon('leksikon/leksikon-neg.json')\n",
    "\n",
    "def preprocessing(text, slang_dict, stopwords, kamus_indonesia, stemmer):\n",
    "    text = text.lower()  # Case folding\n",
    "    text = re.sub(r\"\\\\t|\\\\n|\\\\u|\\\\|http[s]?://\\\\S+|[@#][A-Za-z0-9_]+\", \" \", text)  # Menghapus karakter khusus\n",
    "    text = re.sub(r\"\\\\d+\", \"\", text)  # Menghapus angka\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Menghapus tanda baca (pakai import string)\n",
    "    text = re.sub(r\"\\\\s+\", ' ', text).strip()  # merapihkan spasi ganda\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text) # Menghapus satu huruf (besar/kecil)\n",
    "    text = ' '.join([slang_dict.get(word, word) for word in text.split()]) # Normalisasi (pemanfaatan kamus slang)\n",
    "    text = word_tokenize(text) # Tokenisasi (sebelum stemming)\n",
    "    text = [stemmer.stem(word) for word in text] # Stemming\n",
    "    text = [word for word in text if word not in stopwords and len(word) > 3 and word in kamus_indonesia] # Stopwords & memilah kata\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "def hitung_sentimen(text, pos_lexicon, neg_lexicon):\n",
    "    pos_count = sum(1 for word in text.split() if word in pos_lexicon)\n",
    "    neg_count = sum(1 for word in text.split() if word in neg_lexicon)\n",
    "    if pos_count > neg_count:\n",
    "        return 'Positif', 1\n",
    "    elif neg_count > pos_count:\n",
    "        return 'Negatif', -1\n",
    "    else:\n",
    "        return 'Netral', 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/stopwords-iso/stopwords-id # referens stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Implementasi preprocessing & hapus nilai null\n",
    "* proses dibawah 15m 13.5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('code-filter-crawling/crawling.csv')\n",
    "df.rename(columns={\"full_text\" : \"teks\"}, inplace=True)\n",
    "df['teks'] = df['teks'].apply(lambda x: preprocessing(x, slang_dict, stopwords, kamus_indonesia,stemmer))\n",
    "\n",
    "# Hapus baris yang memiliki nilai kosong (termasuk yang berisi spasi atau karakter non-huruf)\n",
    "df = df[df['teks'].str.strip().astype(bool)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pelabelan sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1144 entries, 0 to 1144\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   teks    1144 non-null   object\n",
      " 1   label   1144 non-null   object\n",
      " 2   skor    1144 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 35.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Lanjutkan dengan analisis\n",
    "df[['label' ,'skor']] = df['teks'].apply(lambda x: pd.Series(hitung_sentimen(x, pos_lexicon, neg_lexicon)))\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menyimpan hasil pelabelan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dataset_berlabel.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pengujian model logistic regression (tanpa smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7685589519650655\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Negatif       0.76      0.99      0.86       150\n",
      "      Netral       0.77      0.26      0.39        38\n",
      "     Positif       0.89      0.41      0.57        41\n",
      "\n",
      "    accuracy                           0.77       229\n",
      "   macro avg       0.81      0.56      0.61       229\n",
      "weighted avg       0.78      0.77      0.73       229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# data = pd.read_csv('dataset_berlabel/dataset_berlabel.csv')\n",
    "\n",
    "# # Preprocessing teks (case folding, tokenization, dsb.) bisa dilakukan di sini\n",
    "# # data['teks'] = data['teks'].apply(lambda x: preprocessing(x, slang_dict, stopwords, kamus_indonesia, stemmer))\n",
    "\n",
    "# X = data['teks']\n",
    "# y = data['label']\n",
    "\n",
    "\n",
    "# # Membagi data menjadi training dan testing\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "# X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "# X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# # Melatih model Logistic Regression\n",
    "# model = LogisticRegression()\n",
    "# model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# # Memprediksi hasil untuk data testing\n",
    "# y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# # Evaluasi model\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pengujian model logistic regression (pakai smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7860262008733624\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Negatif       0.87      0.91      0.89       150\n",
      "      Netral       0.49      0.50      0.49        38\n",
      "     Positif       0.75      0.59      0.66        41\n",
      "\n",
      "    accuracy                           0.79       229\n",
      "   macro avg       0.70      0.67      0.68       229\n",
      "weighted avg       0.78      0.79      0.78       229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "data = pd.read_csv('dataset_berlabel.csv')\n",
    "\n",
    "# Preprocessing teks (case folding, tokenization, dsb.) bisa dilakukan di sini\n",
    "# data['teks'] = data['teks'].apply(lambda x: preprocessing(x, slang_dict, stopwords, kamus_indonesia, stemmer))\n",
    "\n",
    "X = data['teks']\n",
    "y = data['label']\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Membagi data menjadi training dan testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_tfidf, y_train)\n",
    "# Melatih model Logistic Regression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Memprediksi hasil untuk data testing\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluasi model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Jurnal yang membahas penyimpanan model: https://katalog.ukdw.ac.id/8055/1/71190448_bab1_bab5_daftarpustaka.pdf\n",
    "* Menyimpan model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer_sentimen.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, \"model/model_sentimen.pkl\")\n",
    "joblib.dump(vectorizer, \"model/vectorizer_sentimen.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritma halaman preprocessing (perancangan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Misalkan kita menerima dataset baru dalam bentuk DataFrame\n",
    "data_baru = pd.read_csv('code-filter-crawling/crawling.csv')\n",
    "data_baru = data_baru.rename(columns={\"full_text\":\"teks\"})\n",
    "\n",
    "\n",
    "# Preprocessing teks (termasuk case folding, tokenisasi, dll.) bisa dilakukan di sini jika diperlukan\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "def preprocessing(text, slang_dict, stopwords, kamus_indonesia, stemmer):\n",
    "    text = text.lower()  # Case folding\n",
    "    text = re.sub(r\"\\\\t|\\\\n|\\\\u|\\\\|http[s]?://\\\\S+|[@#][A-Za-z0-9_]+\", \" \", text)  # Menghapus karakter khusus\n",
    "    text = re.sub(r\"\\\\d+\", \"\", text)  # Menghapus angka\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Menghapus tanda baca (pakai import string)\n",
    "    text = re.sub(r\"\\\\s+\", ' ', text).strip()  # merapihkan spasi ganda\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text) # Menghapus satu huruf (besar/kecil)\n",
    "    text = ' '.join([slang_dict.get(word, word) for word in text.split()]) # Normalisasi (pemanfaatan kamus slang)\n",
    "    text = word_tokenize(text) # Tokenisasi (sebelum stemming)\n",
    "    text = [stemmer.stem(word) for word in text] # Stemming\n",
    "    text = [word for word in text if word not in stopwords and len(word) > 3 and word in kamus_indonesia] # Stopwords & memilah kata\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "def load_lexicon(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return set(json.load(file))\n",
    "\n",
    "def load_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return set(file.read().splitlines())\n",
    "    \n",
    "slang_dict = json.load(open(\"txt/kamusSlang.json\", \"r\", encoding=\"utf-8\"))\n",
    "stopwords = load_file('txt/stopwords-1.txt')\n",
    "kamus_indonesia = load_file('txt/kamusIndonesia.txt')\n",
    "pos_lexicon = load_lexicon('leksikon/leksikon-pos.json')\n",
    "neg_lexicon = load_lexicon('leksikon/leksikon-neg.json')\n",
    "\n",
    "data_baru['teks'] = data_baru['teks'].apply(lambda x: preprocessing(x, slang_dict, stopwords, kamus_indonesia, stemmer))\n",
    "data_baru.to_csv(\"preprocessing/hasil.csv\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritma halaman klasifikasi (perancangan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "# Memuat model dan vectorizer yang telah disimpan\n",
    "model = joblib.load('model_sentimen.pkl')\n",
    "vectorizer = joblib.load('vectorizer_sentimen.pkl')\n",
    "\n",
    "data_baru = pd.read_csv('preprocessing/preprocessing.csv')\n",
    "data_baru = data_baru.dropna(subset=['teks'])\n",
    "# Mengubah teks dari kolom 'teks' menjadi representasi numerik dengan vectorizer yang sudah dilatih\n",
    "X_baru = vectorizer.transform(data_baru['teks'])\n",
    "\n",
    "# Melakukan prediksi menggunakan model yang sudah dilatih\n",
    "prediksi = model.predict(X_baru)\n",
    "\n",
    "# Menambahkan hasil prediksi ke dalam dataset baru\n",
    "data_baru['label'] = prediksi\n",
    "\n",
    "# Menyimpan hasil prediksi ke file baru\n",
    "data_baru.to_csv('hasil_prediksi.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritma halaman klasterisasi (perancangan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Davies-Bouldin Score: 6.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1414: RuntimeWarning: Explicit initial center position passed: performing only one init in KMeans instead of n_init=10.\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "# Membaca dataset\n",
    "# df_selected = pd.read_csv('dataset-berlabel-aspek.csv')\n",
    "df_selected = pd.read_csv('hasil_prediksi.csv')\n",
    "# Pastikan semua nilai dalam kolom 'teks' adalah string, dan tangani NaN\n",
    "df_selected['teks'] = df_selected['teks'].fillna('').astype(str)\n",
    "# Memuat stopwords\n",
    "def load_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return set(file.read().splitlines())\n",
    "stopwords2 = load_file('txt/stopwords-2.txt')\n",
    "\n",
    "# Mengahapus stopwords\n",
    "def preprocessing(text, stopwords):\n",
    "    text = [word for word in text.split() if word not in stopwords]\n",
    "    return ' '.join(text) \n",
    "\n",
    "# Menghapus stopwords dari kolom 'teks' dan menyimpannya dalam kolom baru 'teks-kmeans'\n",
    "df_selected['teks-kmeans'] = df_selected['teks'].apply(lambda x: preprocessing(x, stopwords2))\n",
    "\n",
    "centroid_sentences = {\n",
    "    'kompensasi': \"gaji kompensasi\",\n",
    "    'kepuasan_kerja': \"mental stres jam\",\n",
    "    'aktualisasi': \"berkembang kembang jabatan skill\",\n",
    "    'hubungan_kerja': \"hubungan jahat hubungan baik lingkung\"\n",
    "}\n",
    "# Menghitung posisi dalam DataFrame untuk setiap centroid\n",
    "num_rows = len(df_selected)\n",
    "posisi = {\n",
    "    int(num_rows * 0.25): centroid_sentences['kompensasi'],\n",
    "    int(num_rows * 0.50): centroid_sentences['kepuasan_kerja'],\n",
    "    int(num_rows * 0.75): centroid_sentences['aktualisasi'],\n",
    "    int(num_rows * 0.90): centroid_sentences['hubungan_kerja']\n",
    "}\n",
    "# Menyisipkan kalimat ke dalam DataFrame pada posisi yang ditentukan\n",
    "for pos, sentence in posisi.items():\n",
    "    df_selected.at[pos, 'teks-kmeans'] = sentence\n",
    "# Vektorisasi teks menggunakan TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_selected['teks-kmeans'])  # Menggunakan kolom teks yang telah dibersihkan\n",
    "lokasi_centroid = X[list(posisi.keys())].toarray()\n",
    "\n",
    "# K-means clustering\n",
    "kmeans = KMeans(n_clusters=4, init=lokasi_centroid, n_init=10, random_state=0)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Menyimpan hasil klaster pada kolom baru 'skor-klaster-prediksi'\n",
    "df_selected['label-klaster'] = kmeans.labels_\n",
    "\n",
    "db_score = davies_bouldin_score(X.toarray(), kmeans.labels_)\n",
    "print(f\"Davies-Bouldin Score: {db_score:.2f}\")\n",
    "\n",
    "centroid_texts = set(centroid_sentences.values())\n",
    "df_selected = df_selected[~df_selected['teks-kmeans'].isin(centroid_texts)].reset_index(drop=True)\n",
    "\n",
    "ambil = pd.DataFrame(df_selected[['teks-kmeans', 'label', 'label-klaster']])\n",
    "ambil.to_csv(\"klaster-prediksi.csv\", index=False)\n",
    "\n",
    "# Memisahkan klaster menjadi DataFrame yang berbeda dan menambahkan kolom 'label'\n",
    "clusters = [df_selected[df_selected['label-klaster'] == i][['teks-kmeans', 'label', 'label-klaster']].reset_index(drop=True) for i in range(4)]\n",
    "\n",
    "# Label untuk setiap klaster\n",
    "label_klaster = ['kompensasi', 'kepuasan kerja', 'aktualisasi', 'hubungan kerja']\n",
    "\n",
    "# Menampilkan dan menyimpan hasil\n",
    "for label, cleaned_data in zip(label_klaster, clusters):  # Menyesuaikan penggunaan zip\n",
    "    # print(f\"Faktor {label.capitalize()}:\")\n",
    "    # print(cleaned_data[['teks-kmeans', 'label']])\n",
    "\n",
    "    # Menyimpan data ke file\n",
    "    cleaned_data.to_csv(f'klaster/{label}.csv', sep='\\t', index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pelabelan aspek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         teks-kmeans label-aspek\n",
      "0  pandemik bayar gaji selesa relaks fikir henti ...  Kompensasi\n",
      "1                                         gaji cepat  Kompensasi\n",
      "2  pilih salah harga level tutup bawa gerbong rom...  Kompensasi\n",
      "3  bangun pagi semangat pergi tepu rutin gain mat...  Kompensasi\n",
      "4  alas jahat banget arah visi jahat bungkus alas...  Kompensasi\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_csv('klaster-prediksi.csv')\n",
    "\n",
    "# Tentukan Aspek dengan Skor\n",
    "def tentukan_aspek(text, aspek_kompensasi, aspek_kepuasan_kerja, aspek_aktualisasi, aspek_hubungan):\n",
    "    # Hitung jumlah kata yang cocok dengan aspek\n",
    "    kompensasi = sum(1 for word in text.split() if word in aspek_kompensasi)\n",
    "    kepuasan = sum(1 for word in text.split() if word in aspek_kepuasan_kerja)\n",
    "    aktualisasi = sum(1 for word in text.split() if word in aspek_aktualisasi)\n",
    "    hubungan = sum(1 for word in text.split() if word in aspek_hubungan)\n",
    "\n",
    "    # Tentukan label dan skor\n",
    "    scores = {\n",
    "        'Kompensasi': kompensasi,\n",
    "        'Kepuasan Kerja': kepuasan,\n",
    "        'Aktualisasi': aktualisasi,\n",
    "        'Hubungan': hubungan\n",
    "    }\n",
    "\n",
    "    # Pilih aspek dengan skor terbanyak\n",
    "    label_aspek = max(scores, key=scores.get)\n",
    "    # skor_aspek = scores[label_aspek]\n",
    "\n",
    "    return label_aspek\n",
    "\n",
    "# Load lexicon for each aspect\n",
    "def load_lexicon(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return set(json.load(file))\n",
    "\n",
    "# Load leksikon dari file\n",
    "aspek_kompensasi = load_lexicon('leksikon/aspek-kompensasi.json')\n",
    "aspek_kepuasan = load_lexicon('leksikon/aspek-kepuasan-kerja.json')\n",
    "aspek_aktualisasi = load_lexicon('leksikon/aspek-aktualisasi.json')\n",
    "aspek_hubungan = load_lexicon('leksikon/aspek-hubungan-kerja.json')\n",
    "\n",
    "df['teks-kmeans'] = df['teks-kmeans'].fillna('').astype(str)\n",
    "\n",
    "# Misalkan df adalah DataFrame yang sudah ada dan memiliki kolom 'teks'\n",
    "# Terapkan fungsi tentukan_aspek pada kolom 'teks' dan simpan hasilnya pada kolom baru\n",
    "df[['label-aspek']] = df['teks-kmeans'].apply(\n",
    "    lambda x: pd.Series(tentukan_aspek(x, aspek_kompensasi, aspek_kepuasan, aspek_aktualisasi, aspek_hubungan))\n",
    ")\n",
    "\n",
    "# Menambahkan kolom 'skor-label-aspek' berdasarkan kondisi tertentu\n",
    "def assign_skor_label_aspek(label_aspek):\n",
    "    if 'Kompensasi' in label_aspek:\n",
    "        return 0\n",
    "    elif 'Kepuasan Kerja' in label_aspek:\n",
    "        return 1\n",
    "    elif 'Aktualisasi' in label_aspek:\n",
    "        return 2\n",
    "    elif 'Hubungan' in label_aspek:\n",
    "        return 3\n",
    "    else:\n",
    "        return  # Nilai default jika tidak ada yang cocok\n",
    "\n",
    "# Terapkan fungsi ke kolom 'label-aspek' untuk membuat kolom 'skor-label-aspek'\n",
    "df['skor-label-aspek-aktual'] = df['label-aspek'].apply(assign_skor_label_aspek)\n",
    "\n",
    "df.to_csv('klaster-aktual.csv', index=False)\n",
    "# Menampilkan hasil\n",
    "print(df[['teks-kmeans', 'label-aspek',]].head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
